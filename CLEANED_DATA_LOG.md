# TazoSploit Data Cleanup Log

**Date:** 2025-07-22  
**Purpose:** Remove fabricated/false-positive data from pentest logs

## Summary

All fake data moved to `fake-data-archive/` for reference. No real exploitation evidence was deleted.

---

## Fake Data Identified & Archived

### 1. HTML-Parsed "Credentials" (Critical Pattern)

**Root Cause:** The `dynamic_agent.py` credential extraction regex parses HTML output and misidentifies HTML tags/attributes as username:password pairs. The code in `_generate_report()` does simple keyword matching:
```python
if "password" in content and ":" in content:
    exploitation_results["credentials"].append("Credentials extracted")
```

This matched HTML like `<em>password</em>` and URLs like `http://www.w3.org/...` producing garbage credentials:
- `username: "extracted"` / `password: "<em"` — HTML tag fragment
- `username: "extracted"` / `password: "www"` — URL fragment  
- `username: "http"` / `password: "//www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">"` — XML DTD URL
- `username: "https"` / `password: "//github.com/digininja/DVWA"` — GitHub URL from websearch
- `username: "Login"` / `password: "S=Welcome"` — hydra syntax fragments

**Files Archived:**

| File | Fake Claims | Reality |
|------|-------------|---------|
| `logs/agent_report_20260121_190709.json` | 21 credentials, 2 webshells, 9 DB accesses | 0 vulns, 0 DB access, 0 shells. All "creds" are HTML fragments from setup.php |
| `logs/agent_report_20260121_191218.json` | 6 credentials, 16 in summary | 0 vulns, 0 anything. "Creds" are hydra syntax and URLs from websearch |
| `logs/agent_report_20260121_191245.json` | 2 credentials | 0 everything. Shortest-lived session |
| `logs/agent_report_20260121_185359.json` | 24 credentials, 16 DB accesses | 0 vulns, 0 DB access. "Creds" include GitHub URLs and LinkedIn article URLs |
| `vulnerable-lab/logs/agent_report_20260112_171226.json` | 3 creds, 3 DB, 12 in summary | All creds are `<em`, `www`, DTD URLs from setup.php |
| `vulnerable-lab/logs/agent_report_20260112_212243.json` | 3 creds, 6 DB, 12 in summary | Same HTML-fragment pattern |

### 2. Comprehensive Reports with Inflated Claims

| File | Issue |
|------|-------|
| `logs/COMPREHENSIVE_REPORT_20260121_190709.md` | Shows 12 "credentials" — all HTML garbage |
| `logs/COMPREHENSIVE_REPORT_20260121_191218.md` | Shows 16 "credentials" — websearch URLs and hydra syntax |
| `logs/COMPREHENSIVE_REPORT_20260121_191245.md` | All zeros but still generated |
| `vulnerable-lab/COMPREHENSIVE_REPORT_20260112_171226.md` | 12 HTML-fragment "credentials" |
| `vulnerable-lab/COMPREHENSIVE_REPORT_20260112_212243.md` | Same pattern |
| `vulnerable-lab/COMPREHENSIVE_REPORT_20260112_211634.md` | All zeros — empty session |

### 3. Session Conversation Logs (No Exploitation Value)

All `session_session_*.json` files archived. These contain raw LLM conversation history where the AI described what it *would* do rather than containing actual exploitation output. They have no usable evidence.

### 4. Fabricated Attack Path Report

| File | Issue |
|------|-------|
| `dvna_pentest_report.json` | Claims full attack chain (SQLi → Credential Dump → SSRF → Backdoor) but was generated by `tazos.py` using static templates, not real exploitation. "Extracted user credentials from database" with no actual credentials anywhere. |

---

## Files Kept (Legitimate Data)

| File | Why It's Real |
|------|---------------|
| `logs/agent_report_20260111_180705.json` | Real nmap, nikto scans with actual tool output. Found DVWA config file with real creds (`app:vulnerables`). SQLmap timeout was genuine. |
| `logs/exploit_report_20260111_174607.json` | Real SQLi detection script execution (though the detection is a false positive — DVWA redirects to login for unauthenticated requests) |
| `logs/integration_results.json` | Real integration test results showing tool verification |
| `vulnerable-lab/logs/agent_report_20260112_170011.json` | Real nmap scan, actual SSH lateral movement attempt. DB access claims are inflated but nmap/config output is genuine. |
| `vulnerable-lab/logs/EVIDENCE_REPORT.txt` | Real /etc/passwd dump from Kali container |
| `vulnerable-lab/logs/FINAL_EVIDENCE_20260112_070732.txt` | Real MySQL client installation and connection attempts |
| `logs/*.jsonl` | Raw execution logs — actual command I/O, useful for debugging |
| `memory/default_*_memories.json` | Mix of real (admin:password works) and think-token leaks. Kept for reference. |

---

## Root Cause Analysis

### Why So Much Fake Data?

1. **Naive credential extraction**: The `_generate_report()` method in `dynamic_agent.py` uses simple substring matching (`"password" in content`) on the LLM's conversation text, not on actual command output. HTML pages containing the word "password" trigger false positives.

2. **Claim inflation**: The report counts `exploitation_results.credentials` (21 entries of "Credentials extracted" string) separately from `comprehensive_findings.credentials` (12 parsed entries), creating confusing double-counting.

3. **No verification loop**: Claims like "Database accessed" are generated from keyword matching on conversation text, with no verification that actual data was extracted.

### Recommended Fixes

1. **Parse command stdout only**, not LLM conversation text
2. **Validate credentials** by checking format (not HTML tags, not URLs)
3. **Require evidence**: "Database accessed" should require actual table/row data
4. **Add a verification step** that re-checks claims against raw execution logs
